{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import moving MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 10000, 64, 64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Moving_MNIST = np.load('data/mnist_test_seq.npy')\n",
    "Moving_MNIST = Moving_MNIST / 255\n",
    "Moving_MNIST.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Give `torch` the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 20, 1, 64, 64])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making into PyTorch tensor\n",
    "Moving_MNIST_tensor = torch.from_numpy(Moving_MNIST)\n",
    "\n",
    "# Putting the existing dimensions into appropriate order\n",
    "Moving_MNIST_tensor = Moving_MNIST_tensor.permute(1, 0, 2, 3)\n",
    "\n",
    "# Added the acknowledge that this is 1 spectral band\n",
    "Moving_MNIST_tensor = Moving_MNIST_tensor.unsqueeze(2)\n",
    "\n",
    "# Checking shape\n",
    "Moving_MNIST_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = np.random.choice(range(10000), size = 8000, replace = False)\n",
    "\n",
    "OutofSample_indices = [index for index in range(10000) if index not in train_indices.tolist()]\n",
    "validation_indices = np.random.choice(OutofSample_indices, size = 1000, replace = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separating $x$ (first $10$ in each sequence) and $y$ (last $10$ in each sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sep_x_y(tensor_to_separate):\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(len(tensor_to_separate)):\n",
    "        current_seq = tensor_to_separate[i, :, :, :, :]\n",
    "        for j in range(current_seq.shape[0]):\n",
    "            if j >= 10:\n",
    "                current_x = current_seq[(j-10):j].numpy()\n",
    "                x.append(current_x)\n",
    "                current_y = current_seq[j].unsqueeze(dim = 0).numpy()\n",
    "                y.append(current_y)\n",
    "    x = np.asarray(x)\n",
    "    x = torch.from_numpy(x).type(torch.FloatTensor)\n",
    "    y = np.asarray(y)\n",
    "    y = torch.from_numpy(y).type(torch.FloatTensor)\n",
    "    return(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = sep_x_y(Moving_MNIST_tensor[train_indices])\n",
    "x_validation, y_validation = sep_x_y(Moving_MNIST_tensor[validation_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viewing the overlaid $(x, y)$ sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random_index = np.random.choice(len(x_validation))\n",
    "\n",
    "for i in range(10):\n",
    "    plt.imshow(x_validation[random_index, i, 0], alpha = 0.25, cmap = 'gist_gray')\n",
    "plt.imshow(y_validation[random_index, 0, 0], cmap = 'Reds', alpha = 0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvTime_LSTMCell(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, input_dim, hidden_dim, kernel_size, bias, GPU):\n",
    "        \"\"\"\n",
    "        Initialize ConvTime_LSTM cell.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        input_size: (int, int)\n",
    "            Height and width of input tensor as (height, width).\n",
    "        input_dim: int\n",
    "            Number of channels of input tensor.\n",
    "        hidden_dim: int\n",
    "            Number of channels of hidden state.\n",
    "        kernel_size: (int, int)\n",
    "            Size of the convolutional kernel.\n",
    "        bias: bool\n",
    "            Whether or not to add the bias.\n",
    "        \"\"\"\n",
    "\n",
    "        super(ConvTime_LSTMCell, self).__init__()\n",
    "\n",
    "        self.height, self.width = input_size\n",
    "        self.input_dim  = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding     = kernel_size[0] // 2, kernel_size[1] // 2\n",
    "        self.bias        = bias\n",
    "        self.GPU         = GPU\n",
    "        \n",
    "        ## Defining the input convolutional layer ##\n",
    "        self.i_conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
    "                                out_channels=self.hidden_dim,\n",
    "                                kernel_size=self.kernel_size,\n",
    "                                padding=self.padding,\n",
    "                                bias=self.bias)\n",
    "        \n",
    "        ## Defining the T2 convolutional layer ##\n",
    "        self.T1_conv = nn.Conv2d(in_channels=self.input_dim,\n",
    "                                 out_channels=self.hidden_dim,\n",
    "                                 kernel_size=self.kernel_size,\n",
    "                                 padding=self.padding,\n",
    "                                 bias=self.bias)\n",
    "        \n",
    "        ## Defining the T1 convolutional layer ##\n",
    "        self.T2_conv = nn.Conv2d(in_channels=self.input_dim,\n",
    "                                 out_channels=self.hidden_dim,\n",
    "                                 kernel_size=self.kernel_size,\n",
    "                                 padding=self.padding,\n",
    "                                 bias=self.bias)\n",
    "        \n",
    "        ## Defining the activation convolutional layer ##\n",
    "        self.c_conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
    "                                out_channels=self.hidden_dim,\n",
    "                                kernel_size=self.kernel_size,\n",
    "                                padding=self.padding,\n",
    "                                bias=self.bias)\n",
    "        \n",
    "        ## Defining the output convolutional layer ##\n",
    "        self.o_conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
    "                                out_channels=self.hidden_dim,\n",
    "                                kernel_size=self.kernel_size,\n",
    "                                padding=self.padding,\n",
    "                                bias=self.bias)\n",
    "        \n",
    "        ## Defining the learnable parameter for time scaling ##\n",
    "        self.constrained_scalar = nn.Parameter(torch.rand(1, 1))\n",
    "        self.unconstrained_scalars = nn.Parameter(torch.rand(2, 1))\n",
    "\n",
    "    def forward(self, input_tensor, time_tensor, cur_state):\n",
    "        \n",
    "        \n",
    "        ## Getting the h_{m-1} and c_{m-1} ##\n",
    "        ##     the previous hidden and activations ##\n",
    "        h_cur, c_cur = cur_state\n",
    "        \n",
    "        \n",
    "        ## Getting the (time tensor)*(their learned scalar) ##\n",
    "        ## For T1, with a constrained weight ##\n",
    "        t_T1 = self.constrained_scalar.clamp(max = 0) * time_tensor\n",
    "        ## For T2 and output gate, without constraints ##\n",
    "        t_bands_list = []\n",
    "        for i in range(2):\n",
    "            unconstrained_scalar = self.unconstrained_scalars[i]\n",
    "            scaled_time_tensors = time_tensor * unconstrained_scalar\n",
    "            t_bands_list.append(scaled_time_tensors)\n",
    "        t_T2, t_o = t_bands_list\n",
    "        ## Applying the appropriate non-linearities to t_T1 and t_T2 ##\n",
    "        t_T1 = torch.sigmoid(t_T1)\n",
    "        t_T2 = torch.sigmoid(t_T2)\n",
    "\n",
    "\n",
    "        ## concatenate the prev. hidden state and the current input along the color channel dim ##\n",
    "        x_h_combined = torch.cat([input_tensor, h_cur], dim = 1)\n",
    "        \n",
    "        \n",
    "        ## The input gate ##\n",
    "        ## Running the input convolution ##\n",
    "        i_conv_outputs = self.i_conv(x_h_combined)\n",
    "        ## Running the input LSTM gate equations ##\n",
    "        i_m = torch.sigmoid(i_conv_outputs)\n",
    "        \n",
    "        \n",
    "        ## The first time gate ##\n",
    "        ## Running the first time convolution ##\n",
    "        T1_conv_outputs = self.T1_conv(input_tensor)\n",
    "        T1_conv_outputs = T1_conv_outputs + t_T1\n",
    "        ## Running the T1 LSTM gate equations ##\n",
    "        T1_m = torch.sigmoid(T1_conv_outputs)\n",
    "        \n",
    "        \n",
    "        ## The second time gate ##\n",
    "        ## Running the second time convolution ##\n",
    "        T2_conv_outputs = self.T2_conv(input_tensor)\n",
    "        T2_conv_outputs = T2_conv_outputs + t_T2\n",
    "        ## Running the T2 LSTM gate equations ##\n",
    "        T2_m = torch.sigmoid(T2_conv_outputs)\n",
    "        \n",
    "        \n",
    "        ## The c vectors ##\n",
    "        ## Running the c convolution ##\n",
    "        c_conv_outputs = self.c_conv(x_h_combined)\n",
    "        ## Computing the c tilde and c activation vectors ##\n",
    "        c_m_tilde = (((1 - i_m * T1_m) * c_cur) +\n",
    "                     (i_m * T1_m * torch.tanh(c_conv_outputs)))\n",
    "        c_m = (((1 - i_m) * c_cur) +\n",
    "               (i_m * T2_m * torch.tanh(c_conv_outputs)))\n",
    "        \n",
    "         \n",
    "        ## The output gate ##\n",
    "        ## Running the output gate convolution\n",
    "        o_conv_output = self.o_conv(x_h_combined)\n",
    "        o_conv_output = o_conv_output + t_o\n",
    "        ## Running the output LSTM gate equations\n",
    "        o_m = torch.sigmoid(o_conv_output)\n",
    "        \n",
    "        \n",
    "        ## The hidden vector ##\n",
    "        h_m = o_m * torch.tanh(c_m_tilde)\n",
    "        \n",
    "        \n",
    "        return h_m, c_m\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        to_return = (Variable(torch.zeros(batch_size, self.hidden_dim, self.height, self.width)),\n",
    "                     Variable(torch.zeros(batch_size, self.hidden_dim, self.height, self.width)))\n",
    "        if self.GPU:\n",
    "            to_return = (to_return[0].cuda(), to_return[1].cuda())\n",
    "        return(to_return)\n",
    "\n",
    "\n",
    "class ConvTime_LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, input_dim, hidden_dim, kernel_size, num_layers,\n",
    "                 batch_first, bias, return_all_layers, GPU):\n",
    "        super(ConvTime_LSTM, self).__init__()\n",
    "\n",
    "        self._check_kernel_size_consistency(kernel_size)\n",
    "\n",
    "        # Make sure that both `kernel_size` and `hidden_dim` are lists having len == num_layers\n",
    "        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\n",
    "        hidden_dim  = self._extend_for_multilayer(hidden_dim, num_layers)\n",
    "        if not len(kernel_size) == len(hidden_dim) == num_layers:\n",
    "            raise ValueError('Inconsistent list length.')\n",
    "\n",
    "        self.height, self.width = input_size\n",
    "\n",
    "        self.input_dim  = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        self.bias = bias\n",
    "        self.return_all_layers = return_all_layers\n",
    "        self.GPU = GPU\n",
    "\n",
    "        cell_list = []\n",
    "        for i in range(0, self.num_layers):\n",
    "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i-1]\n",
    "\n",
    "            cell_list.append(ConvTime_LSTMCell(input_size=(self.height, self.width),\n",
    "                                               input_dim=cur_input_dim,\n",
    "                                               hidden_dim=self.hidden_dim[i],\n",
    "                                               kernel_size=self.kernel_size[i],\n",
    "                                               bias=self.bias,\n",
    "                                               GPU=self.GPU))\n",
    "\n",
    "        self.cell_list = nn.ModuleList(cell_list)\n",
    "\n",
    "    def forward(self, input_tensor, time_tensor, hidden_state=None):\n",
    "        \"\"\"\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        input_tensor: todo \n",
    "            5-D Tensor either of shape (t, b, c, h, w) or (b, t, c, h, w)\n",
    "        hidden_state: todo\n",
    "            None. todo implement stateful\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        last_state_list, layer_output\n",
    "        \"\"\"\n",
    "        \n",
    "        if not self.batch_first:\n",
    "            # (t, b, c, h, w) -> (b, t, c, h, w)\n",
    "            input_tensor.permute(1, 0, 2, 3, 4)\n",
    "\n",
    "        # Implement stateful ConvLSTM\n",
    "        if hidden_state is not None:\n",
    "            raise NotImplementedError()\n",
    "        else:\n",
    "            hidden_state = self._init_hidden(batch_size=input_tensor.size(0))\n",
    "\n",
    "        layer_output_list = []\n",
    "        last_state_list   = []\n",
    "\n",
    "        seq_len = input_tensor.size(1)\n",
    "        cur_layer_input = input_tensor\n",
    "        cur_time_input = time_tensor\n",
    "\n",
    "        for layer_idx in range(self.num_layers):\n",
    "            h, c = hidden_state[layer_idx]\n",
    "            output_inner = []\n",
    "            for t in range(seq_len):\n",
    "\n",
    "                h, c = self.cell_list[layer_idx](input_tensor = cur_layer_input[:, t, :, :, :],\n",
    "                                                 time_tensor = cur_time_input[:, t, :, :, :],\n",
    "                                                 cur_state=[h, c])\n",
    "                output_inner.append(h)\n",
    "\n",
    "            layer_output = torch.stack(output_inner, dim=1)\n",
    "            cur_layer_input = layer_output\n",
    "\n",
    "            layer_output_list.append(layer_output)\n",
    "            last_state_list.append([h, c])\n",
    "\n",
    "        if not self.return_all_layers:\n",
    "            layer_output_list = layer_output_list[-1:]\n",
    "            last_state_list   = last_state_list[-1:]\n",
    "\n",
    "        return layer_output_list, last_state_list\n",
    "\n",
    "    def _init_hidden(self, batch_size):\n",
    "        init_states = []\n",
    "        for i in range(self.num_layers):\n",
    "            init_states.append(self.cell_list[i].init_hidden(batch_size))\n",
    "        return init_states\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_kernel_size_consistency(kernel_size):\n",
    "        if not (isinstance(kernel_size, tuple) or\n",
    "                    (isinstance(kernel_size, list) and all([isinstance(elem, tuple) for elem in kernel_size]))):\n",
    "            raise ValueError('`kernel_size` must be tuple or list of tuples')\n",
    "\n",
    "    @staticmethod\n",
    "    def _extend_for_multilayer(param, num_layers):\n",
    "        if not isinstance(param, list):\n",
    "            param = [param] * num_layers\n",
    "        return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Picking one of the like-sequence tensors within the list to set parameters\n",
    "channels = x.shape[2]\n",
    "height = x.shape[3]\n",
    "width = x.shape[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_time_lstm = ConvTime_LSTM(input_size = (height,\n",
    "                                             width),\n",
    "                               input_dim = channels,\n",
    "                               hidden_dim = [128, 64, 64, 1],\n",
    "                               kernel_size = (5, 5),\n",
    "                               num_layers = 4,\n",
    "                               batch_first = True,\n",
    "                               bias = True,\n",
    "                               return_all_layers = False,\n",
    "                               GPU = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvTime_LSTM(\n",
       "  (cell_list): ModuleList(\n",
       "    (0): ConvTime_LSTMCell(\n",
       "      (i_conv): Conv2d(129, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (T1_conv): Conv2d(1, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (T2_conv): Conv2d(1, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (c_conv): Conv2d(129, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (o_conv): Conv2d(129, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    )\n",
       "    (1): ConvTime_LSTMCell(\n",
       "      (i_conv): Conv2d(192, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (T1_conv): Conv2d(128, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (T2_conv): Conv2d(128, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (c_conv): Conv2d(192, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (o_conv): Conv2d(192, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    )\n",
       "    (2): ConvTime_LSTMCell(\n",
       "      (i_conv): Conv2d(128, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (T1_conv): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (T2_conv): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (c_conv): Conv2d(128, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (o_conv): Conv2d(128, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    )\n",
       "    (3): ConvTime_LSTMCell(\n",
       "      (i_conv): Conv2d(65, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (T1_conv): Conv2d(64, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (T2_conv): Conv2d(64, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (c_conv): Conv2d(65, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (o_conv): Conv2d(65, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_time_lstm.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(conv_time_lstm.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class train_Dataset(data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, data_indices):\n",
    "        'Initialization'\n",
    "        self.data_indices = data_indices\n",
    "    \n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.data_indices)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        IDs = self.data_indices[index]\n",
    "\n",
    "        # Load data and get label\n",
    "        curr_x = x[IDs, :, :, :, :]\n",
    "        curr_y = y[IDs, :, :, :, :]\n",
    "\n",
    "        #return X, y\n",
    "        return(curr_x, curr_y)\n",
    "    \n",
    "class validation_Dataset(data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, data_indices):\n",
    "        'Initialization'\n",
    "        self.data_indices = data_indices\n",
    "    \n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.data_indices)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        IDs = self.data_indices[index]\n",
    "\n",
    "        # Load data and get label\n",
    "        curr_x = x_validation[IDs, :, :, :, :]\n",
    "        curr_y = y_validation[IDs, :, :, :, :]\n",
    "\n",
    "        #return X, y\n",
    "        return(curr_x, curr_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = train_Dataset(data_indices=range(y.shape[0]))\n",
    "validation_set = validation_Dataset(data_indices=range(y_validation.shape[0]))\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = training_set,\n",
    "                                           batch_size = batch_size,\n",
    "                                           shuffle = True)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset = validation_set,\n",
    "                                                batch_size = batch_size,\n",
    "                                                shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_time_lstm = torch.nn.DataParallel(conv_time_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 \n",
      "\tBatch loss:  0.016818838194012642 \n",
      "\n",
      "Epoch:  1 \n",
      "\tBatch loss:  0.013990998268127441 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss_list = []\n",
    "epochs = int(np.ceil((7*10**5) / x.shape[0]))\n",
    "for i in range(epochs):\n",
    "    for data in train_loader:\n",
    "        \n",
    "        # data loader\n",
    "        batch_x, batch_y = data\n",
    "        \n",
    "        # move to GPU\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        \n",
    "        # run model and get the prediction\n",
    "        batch_y_hat = conv_time_lstm(batch_x,\n",
    "                                     torch.ones_like(batch_x))\n",
    "        batch_y_hat = batch_y_hat[0][0][:, -2:-1, :, :, :]\n",
    "        \n",
    "        # calculate and store the loss\n",
    "        batch_loss = loss(batch_y, batch_y_hat)\n",
    "        loss_list.append(batch_loss.item())\n",
    "        \n",
    "        # update parameters\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print('Epoch: ', i, '\\n\\tBatch loss: ', batch_loss.item(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_list,\n",
    "         color = 'cyan',\n",
    "         label = 'Batch')\n",
    "plt.plot(np.convolve(loss_list, 1/10*np.ones(10))[10:-10],\n",
    "         color = 'navy',\n",
    "         label = 'Running average')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting random predictions for the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_x, rand_y = next(iter(validation_loader))\n",
    "\n",
    "rand_y_hat = conv_time_lstm(rand_x.to(device),\n",
    "                            torch.ones_like(rand_x).to(device))[0][0][:, -2:-1, :, :, :]\n",
    "rand_y_hat = rand_y_hat.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viewing random predictions for the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_random_validation_pred():\n",
    "    f, axarr = plt.subplots(1, 2)\n",
    "    f.set_figheight(4)\n",
    "    f.set_figwidth(8)\n",
    "    random_index = np.random.choice(len(rand_x))\n",
    "    for i in range(10):\n",
    "        axarr[0].imshow(rand_x[random_index, i, 0], alpha = 0.25, cmap = 'gist_gray')\n",
    "        axarr[1].imshow(rand_x[random_index, i, 0], alpha = 0.25, cmap = 'gist_gray')\n",
    "    axarr[0].imshow(rand_y[random_index, 0, 0], cmap = 'Reds', alpha = 0.5)\n",
    "    axarr[0].set_title('Red = True', fontsize = 15)\n",
    "    axarr[1].imshow(rand_y_hat[random_index, 0, 0], cmap = 'Greens', alpha = 0.5)\n",
    "    axarr[1].set_title('Green = Predicted', fontsize = 15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_random_validation_pred()\n",
    "plot_random_validation_pred()\n",
    "plot_random_validation_pred()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_pred_true():\n",
    "    random_index = np.random.choice(len(rand_x))\n",
    "    plt.imshow(rand_y[random_index, 0, 0], cmap = 'Reds', alpha = 0.5)\n",
    "    plt.imshow(rand_y_hat[random_index, 0, 0], cmap = 'Greens', alpha = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlay_pred_true()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mimicking sequence-to-sequence by shuffling in predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "random_index = np.random.choice(len(rand_x) - 1)\n",
    "rand_x = rand_x[random_index:(random_index+1)]\n",
    "\n",
    "for i in range(10):\n",
    "    f, axarr = plt.subplots(1, 2)\n",
    "    f.set_figheight(3)\n",
    "    f.set_figwidth(6)\n",
    "    axarr[0].imshow(rand_x[0, 0, 0], cmap = 'gist_gray')\n",
    "    rand_y_hat = conv_time_lstm(rand_x.to(device),\n",
    "                                torch.ones_like(rand_x.to(device)))[0][0][:, -2:-1, :, :, :]\n",
    "    axarr[1].imshow(rand_y_hat[0, 0, 0].data.cpu().numpy(), cmap = 'Greens')\n",
    "    rand_x = torch.cat([rand_x, rand_y_hat.data.cpu()], dim = 1)\n",
    "    rand_x = rand_x[:, 1:]\n",
    "    plt.pause(0.01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
